{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a40989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.scalers import *\n",
    "from src.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b0508",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1fb776",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(pd.read_csv('../input/digit-recognizer/train.csv'))\n",
    "dataset = np.array(dataset, dtype='float64')\n",
    "X, y = dataset[:, 1:], dataset[:, 0, np.newaxis]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29159dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y):\n",
    "    yy = np.zeros(shape=[10, y.shape[0]])\n",
    "    for i in range(y.shape[0]):\n",
    "        yy[int(y[i]), i] = 1.\n",
    "    return X.T, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef822bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_data(x_train, y_train)\n",
    "x_test, y_test = preprocess_data(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025dbc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>37790</th>\n",
       "      <th>37791</th>\n",
       "      <th>37792</th>\n",
       "      <th>37793</th>\n",
       "      <th>37794</th>\n",
       "      <th>37795</th>\n",
       "      <th>37796</th>\n",
       "      <th>37797</th>\n",
       "      <th>37798</th>\n",
       "      <th>37799</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 37800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
       "2    0.0    0.0    0.0    1.0    1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0  ...   \n",
       "5    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "6    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  ...   \n",
       "7    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0  ...   \n",
       "8    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "9    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "\n",
       "   37790  37791  37792  37793  37794  37795  37796  37797  37798  37799  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  \n",
       "1    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "5    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0  \n",
       "7    0.0    1.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "8    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "9    0.0    0.0    0.0    0.0    0.0    1.0    1.0    0.0    0.0    0.0  \n",
       "\n",
       "[10 rows x 37800 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65564022",
   "metadata": {},
   "source": [
    "## Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12824e5f",
   "metadata": {},
   "source": [
    "* ***Activation function (sigmoid)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b793fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = 1 / (1 + np.exp(-Z))\n",
    "    sigmoid_memory = Z\n",
    "    \n",
    "    return H, sigmoid_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e54650",
   "metadata": {},
   "source": [
    "* ***Activation function (ReLU)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b81db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # relu_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = np.maximum(0, Z)\n",
    "    \n",
    "    assert(H.shape == Z.shape)\n",
    "    \n",
    "    relu_memory = Z\n",
    "    return H, relu_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d49da",
   "metadata": {},
   "source": [
    "* ***Activation function (softmax)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b66ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # softmax_memory is stored as it is used later on in backpropagation\n",
    "   \n",
    "    Z_exp = np.exp(Z)\n",
    "\n",
    "    Z_sum = np.sum(Z_exp, axis=0, keepdims=True)\n",
    "    \n",
    "    H = Z_exp / Z_sum  # normalising step\n",
    "    softmax_memory = Z\n",
    "    \n",
    "    return H, softmax_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfb537",
   "metadata": {},
   "source": [
    "### Initialize parameters\n",
    "\n",
    "The inputs to this function is a list named dimensions.\n",
    "The length of the list is the number layers in the network + 1 (the plus one is for the input layer, rest are hidden + output).\n",
    "The first element of this list is the dimensionality or length of the input (784 for the MNIST dataset).\n",
    "The rest of the list contains the number of neurons in the corresponding (hidden and output) layers.\n",
    "\n",
    "For example dimensions = [784, 3, 7, 10] specifies a network for the MNIST dataset with two hidden layers and a 10-dimensional softmax output.\n",
    "\n",
    "Parameters are returned in a dictionary.\n",
    "This will help you in implementing the feedforward through the layer and the backprop throught the layer at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622921e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "\n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)            # number of layers in the network + 1\n",
    "\n",
    "    for l in range(1, L): \n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    " \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59699fea",
   "metadata": {},
   "source": [
    "### Layer forward\n",
    "\n",
    "layer_forward implements the forward propagation for a certain layer.\n",
    "It calculates the cumulative input into the layer Z and uses it to calculate the output of the layer H.\n",
    "It takes H_prev, W, b and the activation function as inputs and stores the linear_memory, activation_memory in the variable memory which will be used later in backpropagation.\n",
    "\n",
    "\n",
    "You have to first calculate the Z(using the forward propagation equation), linear_memory(H_prev, W, b) and then calculate H, activation_memory(Z) by applying activation functions - sigmoid, relu and softmax on Z.\n",
    "\n",
    "\n",
    "Note that$$H^{L-1}$$is referred here as H_prev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d00a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_forward(H_prev, W, b, activation = 'relu'):\n",
    "\n",
    "    # H_prev is of shape (size of previous layer, number of examples)\n",
    "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "    # b is bias vector of shape (size of the current layer, 1)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function \n",
    "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z = np.add(np.dot(W, H_prev), b)\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = sigmoid(Z)\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = np.add(np.dot(W, H_prev), b)\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z = np.add(np.dot(W, H_prev) ,b)\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z)\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b4cf18",
   "metadata": {},
   "source": [
    "### L layer forward\n",
    "\n",
    "L_layer_forward performs one forward pass through the whole network for all the training samples (in one single batch). \n",
    "Use the layer_forward you have created above here to perform the feedforward for layers 1 to 'L-1' in the for loop with the activation relu. \n",
    "The last layer having a different activation softmax is calculated outside the loop. \n",
    "Notice that the memory is appended to memories for all the layers. These will be used in the backward order during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e522299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev = H #write your code here \n",
    "        \n",
    "        H, memory = layer_forward(H_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation=\"relu\")\n",
    "        \n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    HL, memory = layer_forward(H, parameters['W' + str(l+1)], parameters['b' + str(l+1)], activation=\"softmax\")\n",
    "    \n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046da55",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "compute_loss here calculates the cross-entropy loss.\n",
    "It is the average loss across all the data points in the batch.\n",
    "It takes the output of the last layer HL and the ground truth label Y as input and returns the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f8b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(HL, Y):\n",
    "\n",
    "    # HL is probability matrix of shape (10, number of examples)\n",
    "    # Y is true \"label\" vector shape (10, number of examples)\n",
    "\n",
    "    # loss is the cross-entropy loss\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    loss = -1. / m * np.sum(np.multiply(Y, np.log(HL)))\n",
    "    \n",
    "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711bb055",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d85d9",
   "metadata": {},
   "source": [
    "* ***Sigmoid backward***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2346c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dH, sigmoid_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a sigmoid function\n",
    "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = sigmoid_memory\n",
    "    \n",
    "    H = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dH * H * (1 - H)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d520d1",
   "metadata": {},
   "source": [
    "* ***ReLU backward***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea6b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dH, relu_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a relu function\n",
    "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
    "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = relu_memory\n",
    "    dZ = np.array(dH, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cae17",
   "metadata": {},
   "source": [
    "### Layer backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9168fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_backward(dH, memory, activation = 'relu'):\n",
    "    \n",
    "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "    # performs the backprop depending upon the activation function\n",
    "    \n",
    "\n",
    "    linear_memory, activation_memory = memory\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = np.dot(dZ, H_prev.T) * (1. / m)\n",
    "        db = np.sum(dZ * (1. / m), axis=1).reshape((activation_memory.shape[0], 1))\n",
    "        dH_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = relu_sigmoid(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = np.dot(dZ, H_prev.T)*(1. / m)\n",
    "        db =  np.sum(dZ * (1. / m), axis=1).reshape((activation_memory.shape[0], 1))\n",
    "        dH_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dH_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d0997",
   "metadata": {},
   "source": [
    "### L layer backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95a13fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Takes the predicted value HL and the true target value Y and the \n",
    "    # memories calculated by L_layer_forward as input\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    m = HL.shape[1]\n",
    "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    # Use the expressions you have used in 'layer_backward'\n",
    "    gradients[\"dH\" + str(L - 1)] = np.dot(W.T, dZ)\n",
    "    gradients[\"dW\" + str(L)] =  np.dot(dZ, H_prev.T) * (1. / m)\n",
    "    gradients[\"db\" + str(L)] = np.sum(dZ * (1. / m), axis=1).reshape((activation_memory.shape[0], 1))\n",
    "    \n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L - 1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\" + str(l + 1)], current_memory, activation = 'relu')\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f717c",
   "metadata": {},
   "source": [
    "## Parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b63ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - (learning_rate* gradients[\"dW\" + str(l + 1)])\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - (learning_rate* gradients[\"db\" + str(l + 1)])\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2eae5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [784, 45, 10]  # three-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04056934",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ebc3b",
   "metadata": {},
   "source": [
    "### L layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ea08d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    \n",
    "    # X and Y are the input training datasets\n",
    "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
    "    # returns updated parameters\n",
    "\n",
    "    np.random.seed(2)\n",
    "    losses = []  # keep track of loss\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        HL, memories = L_layer_forward(X, parameters)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        gradients = L_layer_backward(HL, Y, memories)\n",
    " \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "                \n",
    "        # Printing the loss every 100 training example\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            losses.append(loss)\n",
    "            \n",
    "    # Plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3598a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: nan\n",
      "Loss after iteration 100: nan\n",
      "Loss after iteration 200: nan\n",
      "Loss after iteration 300: nan\n",
      "Loss after iteration 400: nan\n",
      "Loss after iteration 500: nan\n",
      "Loss after iteration 600: nan\n",
      "Loss after iteration 700: nan\n",
      "Loss after iteration 800: nan\n",
      "Loss after iteration 900: nan\n",
      "Loss after iteration 1000: nan\n",
      "Loss after iteration 1100: nan\n",
      "Loss after iteration 1200: nan\n",
      "Loss after iteration 1300: nan\n",
      "Loss after iteration 1400: nan\n",
      "Loss after iteration 1500: nan\n",
      "Loss after iteration 1600: nan\n",
      "Loss after iteration 1700: nan\n",
      "Loss after iteration 1800: nan\n",
      "Loss after iteration 1900: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYXElEQVR4nO3de5RlZX3m8e9DI+IFQaRBoNEm2o5iRg1Toi5khoTLogkBEm94g8GZhThhoskog2JczqyYQUlGZQ0DYRkDJESCF0bURkDGWzQoBYFGRKQlAi2NtIwiFxVbf/PH2SWnj6eqT7/VVae66/tZa686+93v3vv31umu5+zLOSdVhSRJm2u7cRcgSdo6GSCSpCYGiCSpiQEiSWpigEiSmhggkqQmBogWtSQHJbl13HVIWyMDRGOT5LtJDh1nDVX15ar6V+OsYUqSg5Osnad9HZLkW0keTvL5JE+foe+uSS5N8lCSO5K8ZtRtJbk8yYN90yNJbupb/t0kP+lbfuXcjFhzwQDRNi3JknHXAJCeBfH/LcluwCeAPwV2BSaBf5hhlbOBR4A9gNcC5yR57ijbqqqVVfXEqQn4KvDRge3/Xl+fw7fEGDU/FsQ/aKlfku2SnJbkO0nuS3JJkl37ln80yT1J7k/ypak/Zt2y85Ock2RVkoeA3+5e5b41yepunX9IsmPXf6NX/TP17ZafmmRdkruT/MckleSZ04zjC0nek+QrwMPAbyQ5McktSR5IcnuSN3Z9nwBcDuzV92p8r039Lhr9AXBzVX20qn4KvBt4fpJnDxnDE4CXAX9aVQ9W1T8ClwGvb9jWcuAg4G9nWb8WCANEC9EfAccC/w7YC/ghvVfBUy4HVgC7A9cDFw2s/xrgPcBOwD92ba8EjgD2BZ4H/PsZ9j+0b5IjgD8BDgWe2dW3Ka8HTupquQO4FzgKeBJwIvD+JPtX1UPASuDuvlfjd4/wu/iVJE9L8qMZpqlTT88Fbpxar9v3d7r2Qc8CflFV3+5ru7Gv7+Zs63jgy1X1LwPtFyVZn+TKJM8fNjYtTNuPuwBpiDcCp1TVWoAk7wbuTPL6qtpQVR+e6tgt+2GSnavq/q75k1X1le7xT5MAnNX9QSbJp4AXzLD/6fq+Evibqrq5W/bfgNdtYiznT/XvfKbv8Re7c/4H0QvCYWb8XfR3rKo7gV02UQ/AE4H1A2330wu5YX3vn6Hv5mzreODPBtpeS2/sAd4MXJHk2VX1oxnq1wLhEYgWoqcDl069cgZuAX4B7JFkSZIzulM6Pwa+262zW9/6dw3Z5j19jx+m94dvOtP13Wtg28P2M2ijPklWJrkmyf/rxnYkG9c+aNrfxQj7ns6D9I6A+j0JeKCh70jbSvJS4KnAx/rbq+orVfWTqnq4qv4H8CN6gaqtgAGiheguYGVV7dI37VhV36N3euoYeqeRdgaWd+ukb/25+ojpdcCyvvl9RljnV7UkeSzwceAvgD2qahdgFY/WPqzumX4XG+lOYT04w/TaruvNwPP71nsC8IyufdC3ge2TrOhre35f31G3dQLwiap6cMg++hUbP5dawAwQjdtjkuzYN20PnAu8J93toEmWJjmm678T8DPgPuDxwJ/PY62XACcmeU6SxwPv2sz1dwAeS++Uz4YkK4H+u46+Dzwlyc59bTP9LjZSVXf23/E0ZJq6VnQp8JtJXtbdIPAuYHVVfWvINh+id5fVf0/yhCQH0gvwvx11W0keB7wCOL9/213gHZhkh+65fxu9o7GvoK2CAaJxWwX8pG96N/BBenf6XJnkAeAa4EVd/wvpXYz+HvDNbtm8qKrLgbOAzwNrgH/qFv1sxPUfoHdR/BJ6F8NfQ2+cU8u/BXwEuL07ZbUXM/8uWsexnt6dVe/p6ngRcNzU8iTvSHJ53yr/CXgcvRsAPgK8aeq6zqa21TmW3nWRzw+07wSc0633PXo3LqysqvtmMz7Nn/iFUlKbJM8BvgE8dvCCtrQYeAQibYYkv9+dcnky8F7gU4aHFisDRNo8b6R3DeM79O6GetN4y5HGx1NYkqQmHoFIkposqnei77bbbrV8+fJxlyFJW5XrrrvuB1W1dLB9UQXI8uXLmZycHHcZkrRVSXLHsHZPYUmSmhggkqQmBogkqYkBIklqYoBIkpoYIJKkJgaIJKmJASJJamKASJKaGCCSpCYGiCSpiQEiSWpigEiSmhggkqQmBogkqYkBIklqYoBIkpoYIJKkJgaIJKmJASJJamKASJKaGCCSpCYGiCSpiQEiSWpigEiSmow1QJIckeTWJGuSnDZkeZKc1S1fnWT/geVLkvxzkk/PX9WSJBhjgCRZApwNrAT2A16dZL+BbiuBFd10EnDOwPI3A7fMcamSpCHGeQRyALCmqm6vqkeAi4FjBvocA1xYPdcAuyTZEyDJMuB3gQ/NZ9GSpJ5xBsjewF1982u7tlH7fAA4FfjlTDtJclKSySST69evn1XBkqRHjTNAMqStRumT5Cjg3qq6blM7qarzqmqiqiaWLl3aUqckaYhxBshaYJ+++WXA3SP2ORA4Osl36Z36+p0kfzd3pUqSBo0zQK4FViTZN8kOwHHAZQN9LgOO7+7GejFwf1Wtq6q3V9Wyqlrerfd/q+p181q9JC1y249rx1W1IckpwBXAEuDDVXVzkpO75ecCq4AjgTXAw8CJ46pXkrSxVA1edth2TUxM1OTk5LjLkKStSpLrqmpisN13okuSmhggkqQmBogkqYkBIklqYoBIkpoYIJKkJgaIJKmJASJJamKASJKaGCCSpCYGiCSpiQEiSWpigEiSmhggkqQmBogkqYkBIklqYoBIkpoYIJKkJgaIJKmJASJJamKASJKaGCCSpCYGiCSpiQEiSWpigEiSmhggkqQmBogkqYkBIklqYoBIkpoYIJKkJmMNkCRHJLk1yZokpw1ZniRndctXJ9m/a98nyeeT3JLk5iRvnv/qJWlxG1uAJFkCnA2sBPYDXp1kv4FuK4EV3XQScE7XvgH4L1X1HODFwB8OWVeSNIfGeQRyALCmqm6vqkeAi4FjBvocA1xYPdcAuyTZs6rWVdX1AFX1AHALsPd8Fi9Ji904A2Rv4K6++bX8eghssk+S5cBvAV/b8iVKkqYzzgDJkLbanD5Jngh8HHhLVf146E6Sk5JMJplcv359c7GSpI2NM0DWAvv0zS8D7h61T5LH0AuPi6rqE9PtpKrOq6qJqppYunTpFilckjTeALkWWJFk3yQ7AMcBlw30uQw4vrsb68XA/VW1LkmAvwZuqar/Ob9lS5IAth/XjqtqQ5JTgCuAJcCHq+rmJCd3y88FVgFHAmuAh4ETu9UPBF4P3JTkhq7tHVW1ah6HIEmLWqoGLztsuyYmJmpycnLcZUjSViXJdVU1MdjuO9ElSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNRkpQJK8OcmT0vPXSa5PcvhcFydJWrhGPQJ5Q1X9GDgcWAqcCJwxZ1VJkha8UQMk3c8jgb+pqhv72iRJi9CoAXJdkivpBcgVSXYCfjnbnSc5IsmtSdYkOW3I8iQ5q1u+Osn+o64rSZpb24/Y7z8ALwBur6qHk+xK7zRWsyRLgLOBw4C1wLVJLquqb/Z1Wwms6KYXAecALxpxXUnSHBr1COQlwK1V9aMkrwPeCdw/y30fAKypqtur6hHgYuCYgT7HABdWzzXALkn2HHFdSdIcGjVAzgEeTvJ84FTgDuDCWe57b+Cuvvm1XdsofUZZF4AkJyWZTDK5fv36WZYsSZoyaoBsqKqi9yr/g1X1QWCnWe572EX4GrHPKOv2GqvOq6qJqppYunTpZpYoSZrOqNdAHkjyduD1wEHdNYjHzHLfa4F9+uaXAXeP2GeHEdaVJM2hUY9AXgX8jN77Qe6hd7rozFnu+1pgRZJ9k+wAHAdcNtDnMuD47m6sFwP3V9W6EdeVJM2hkY5AquqeJBcBL0xyFPD1qprVNZCq2pDkFOAKYAnw4aq6OcnJ3fJzgVX0bh1eAzxMd+fXdOvOph5J0uZJ79LGJjolr6R3xPEFetcfDgLeVlUfm9PqtrCJiYmanJwcdxmStFVJcl1VTQy2j3oN5HTghVV1b7expcDngK0qQCRJW86o10C2mwqPzn2bsa4kaRs06hHIZ5NcAXykm38VvesTkqRFatSL6G9L8jLgQHrXQM6rqkvntDJJ0oI26hEIVfVx4ONzWIskaSsyY4AkeYDh7/AOUFX1pDmpSpK04M0YIFU1248rkSRto7yTSpLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNxhIgSXZNclWS27qfT56m3xFJbk2yJslpfe1nJvlWktVJLk2yy7wVL0kCxncEchpwdVWtAK7u5jeSZAlwNrAS2A94dZL9usVXAb9ZVc8Dvg28fV6qliT9yrgC5Bjggu7xBcCxQ/ocAKypqtur6hHg4m49qurKqtrQ9bsGWDa35UqSBo0rQPaoqnUA3c/dh/TZG7irb35t1zboDcDlW7xCSdKMtp+rDSf5HPDUIYtOH3UTQ9pqYB+nAxuAi2ao4yTgJICnPe1pI+5akrQpcxYgVXXodMuSfD/JnlW1LsmewL1Duq0F9umbXwbc3beNE4CjgEOqqphGVZ0HnAcwMTExbT9J0uYZ1ymsy4ATuscnAJ8c0udaYEWSfZPsABzXrUeSI4D/ChxdVQ/PQ72SpAHjCpAzgMOS3AYc1s2TZK8kqwC6i+SnAFcAtwCXVNXN3fr/C9gJuCrJDUnOne8BSNJiN2ensGZSVfcBhwxpvxs4sm9+FbBqSL9nzmmBkqRN8p3okqQmBogkqYkBIklqYoBIkpoYIJKkJgaIJKmJASJJamKASJKaGCCSpCYGiCSpiQEiSWpigEiSmhggkqQmBogkqYkBIklqYoBIkpoYIJKkJgaIJKmJASJJamKASJKaGCCSpCYGiCSpiQEiSWpigEiSmhggkqQmBogkqYkBIklqYoBIkpoYIJKkJgaIJKmJASJJajKWAEmya5KrktzW/XzyNP2OSHJrkjVJThuy/K1JKsluc1+1JKnfuI5ATgOurqoVwNXd/EaSLAHOBlYC+wGvTrJf3/J9gMOAO+elYknSRsYVIMcAF3SPLwCOHdLnAGBNVd1eVY8AF3frTXk/cCpQc1inJGka4wqQPapqHUD3c/chffYG7uqbX9u1keRo4HtVdeOmdpTkpCSTSSbXr18/+8olSQBsP1cbTvI54KlDFp0+6iaGtFWSx3fbOHyUjVTVecB5ABMTEx6tSNIWMmcBUlWHTrcsyfeT7FlV65LsCdw7pNtaYJ+++WXA3cAzgH2BG5NMtV+f5ICqumeLDUCSNKNxncK6DDihe3wC8Mkhfa4FViTZN8kOwHHAZVV1U1XtXlXLq2o5vaDZ3/CQpPk1rgA5AzgsyW307qQ6AyDJXklWAVTVBuAU4ArgFuCSqrp5TPVKkgbM2SmsmVTVfcAhQ9rvBo7sm18FrNrEtpZv6fokSZvmO9ElSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1SVWNu4Z5k2Q9cMe462iwG/CDcRcxjxbbeMExLxZb65ifXlVLBxsXVYBsrZJMVtXEuOuYL4ttvOCYF4ttbcyewpIkNTFAJElNDJCtw3njLmCeLbbxgmNeLLapMXsNRJLUxCMQSVITA0SS1MQAWQCS7JrkqiS3dT+fPE2/I5LcmmRNktOGLH9rkkqy29xXPTuzHXOSM5N8K8nqJJcm2WXeit9MIzxvSXJWt3x1kv1HXXehah1zkn2SfD7JLUluTvLm+a++zWye5275kiT/nOTT81f1LFWV05gn4H3Aad3j04D3DumzBPgO8BvADsCNwH59y/cBrqD3Rsndxj2muR4zcDiwfff4vcPWXwjTpp63rs+RwOVAgBcDXxt13YU4zXLMewL7d493Ar69rY+5b/mfAH8PfHrc4xl18ghkYTgGuKB7fAFw7JA+BwBrqur2qnoEuLhbb8r7gVOBreWuiFmNuaqurKoNXb9rgGVzW26zTT1vdPMXVs81wC5J9hxx3YWoecxVta6qrgeoqgeAW4C957P4RrN5nkmyDPhd4EPzWfRsGSALwx5VtQ6g+7n7kD57A3f1za/t2khyNPC9qrpxrgvdgmY15gFvoPfKbiEaZQzT9Rl1/AvNbMb8K0mWA78FfG3Ll7jFzXbMH6D3AvCXc1TfnNh+3AUsFkk+Bzx1yKLTR93EkLZK8vhuG4e31jZX5mrMA/s4HdgAXLR51c2bTY5hhj6jrLsQzWbMvYXJE4GPA2+pqh9vwdrmSvOYkxwF3FtV1yU5eEsXNpcMkHlSVYdOtyzJ96cO37tD2nuHdFtL7zrHlGXA3cAzgH2BG5NMtV+f5ICqumeLDaDBHI55ahsnAEcBh1R3EnkBmnEMm+izwwjrLkSzGTNJHkMvPC6qqk/MYZ1b0mzG/HLg6CRHAjsCT0ryd1X1ujmsd8sY90UYpwI4k40vKL9vSJ/tgdvphcXURbrnDun3XbaOi+izGjNwBPBNYOm4x7KJcW7yeaN37rv/4urXN+c5X2jTLMcc4ELgA+Mex3yNeaDPwWxFF9HHXoBTATwFuBq4rfu5a9e+F7Cqr9+R9O5K+Q5w+jTb2loCZFZjBtbQO598QzedO+4xzTDWXxsDcDJwcvc4wNnd8puAic15zhfi1Dpm4KX0Tv2s7ntujxz3eOb6ee7bxlYVIH6UiSSpiXdhSZKaGCCSpCYGiCSpiQEiSWpigEiSmhgg2iYk+Wr3c3mS12zhbb9j2L7mSpJjk7xrjrb9jk332uxt/usk52/p7Wrh8zZebVO6j4J4a1UdtRnrLKmqX8yw/MGqeuIWKG/Uer4KHF1VP5jldn5tXHM1lu5ja95QVXdu6W1r4fIIRNuEJA92D88ADkpyQ5I/7r5j4cwk13bfwfDGrv/B3fdO/D29N3WR5P8kua77HoqTurYzgMd127uof1/d9zucmeQbSW5K8qq+bX8hyce67yy5KN3nzCQ5I8k3u1r+Ysg4ngX8bCo8kpyf5NwkX07y7e5zk6a+O2KkcfVte9hYXpfk613bXyVZMjXGJO9JcmOSa5Ls0bW/ohvvjUm+1Lf5TwHHzeIp1NZo3O9kdHLaEhPwYPfzYPreyQucBLyze/xYYJLex00cDDwE7NvXd+rd8I8DvgE8pX/bQ/b1MuAqet8FsQdwJ73vszgYuJ/eZx1tB/wTvXdY7wrcyqNH/rsMGceJwF/2zZ8PfLbbzgp6n6e04+aMa1jt3ePn0PvD/5hu/n8Dx3ePC/i97vH7+vZ1E7D3YP3AgcCnxv3vwGl+Jz9MUdu6w4HnJXl5N78zvT/Ej9D7LKJ/6ev7R0l+v3u8T9fvvhm2/VLgI9U7TfT9JF8EXgj8uNv2WoAkNwDL6X1vyU+BDyX5DDDsm+f2BNYPtF1SVb8EbktyO/DszRzXdA4B/g1wbXeA9Dge/VDLR/rquw44rHv8FeD8JJcA/R90eC+9j6HRImKAaFsX4D9X1RUbNfaulTw0MH8o8JKqejjJF+i90t/Utqfzs77Hv6D37YkbkhxA7w/3ccApwO8MrPcTemHQb/BC5dRHvW9yXJsQ4IKqevuQZT+vqqn9/oLub0VVnZzkRfQ+GPCGJC+oqvvo/a5+MuJ+tY3wGoi2NQ/Q+yrUKVcAb+o+Ipwkz0ryhCHr7Qz8sAuPZ9P7tNQpP59af8CXgFd11yOWAv8W+Pp0hXXfcbFzVa0C3gK8YEi3W4BnDrS9Isl2SZ5B7ytTb92McQ3qH8vVwMuT7N5tY9ckT59p5STPqKqvVdW7gB/w6MeTP4veaT8tIh6BaFuzGtiQ5EZ61w8+SO/00fXdhez1DP/63M8CJydZTe8P9DV9y84DVie5vqpe29d+KfASeh/dXcCpVXVPF0DD7AR8MsmO9F79//GQPl8C/jJJ+o4AbgW+SO86y8lV9dMkHxpxXIM2GkuSdwJXJtkO+Dnwh8AdM6x/ZpIVXf1Xd2MH+G3gMyPsX9sQb+OVFpgkH6R3Qfpz6b2/4tNV9bExlzWtJI+lF3AvrUe/p16LgKewpIXnz4HHj7uIzfA0el8OZngsMh6BSJKaeAQiSWpigEiSmhggkqQmBogkqYkBIklq8v8BxZI0TbIO0T0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(x_train, y_train, dimensions, num_iterations = 2000, print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6df5a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    # Performs forward propogation using the trained parameters and calculates the accuracy\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2  # number of layers in the neural network\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_layer_forward(X, parameters)\n",
    "    \n",
    "    p = np.argmax(probas, axis = 0)\n",
    "    act = np.argmax(y, axis = 0)\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67de4eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09714285714285714\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(x_test, y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddcffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
